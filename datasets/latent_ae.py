import numpy as np
import arcadian.dataset
import os
import h5py

class LatentDataset(arcadian.dataset.Dataset):
    def __init__(self, save_dir, latent_size, data=None, autoencoder=None, regenerate=False,
                 conversion_batch_size=50, max_codes=None, feature_name='code'):
        """Dataset of latent codes generated by an autoencoder for some input data examples.
        Produces feature 'code' to represent generated latent vector of autoencoder.

        Arguments:
            - save_dir: where to save results for faster reloading
            - latent_size: dimension of autoencoder intermediate vector
            - data: dataset that the autoencoder takes as input to produce codes
            - autoencoder: model that can evaluate a 'code' feature for input data
            - regenerate: if True, always regenerate codes instead of loading from save_dir
            - conversion_batch_size: batch size to use when generating codes
            - max_codes: max number of codes to use in dataset

        Only specify autoencoder and data if regenerating dataset.
        """
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        self.feature_name = feature_name  # what to name dataset
        self.latent_code_dataset_filename = os.path.join(save_dir, 'ukwac_codes.hdf5')

        # If the codes haven't been generated, or we wish to regenerate them, convert.
        if regenerate or not os.path.isfile(self.latent_code_dataset_filename):
            self.dataset_file = h5py.File(self.latent_code_dataset_filename, 'w')
            print('LatentDataset: Generating latent codes...')
            dataset = self.dataset_file.create_dataset('codes', (len(data), latent_size), dtype='float32')
            index_in_dataset = 0
            for ukwac_batch in data.generate_batches(batch_size=conversion_batch_size):
                np_batch_codes = autoencoder.predict(ukwac_batch, outputs=['code'], batch_size=conversion_batch_size)
                dataset[index_in_dataset:index_in_dataset+np_batch_codes.shape[0], :] = np_batch_codes
                index_in_dataset += np_batch_codes.shape[0]
        else:
            print('LatentDataset: Codes already exist. Loading from file')
            self.dataset_file = h5py.File(self.latent_code_dataset_filename, 'r+')

        self.dataset = self.dataset_file['codes']
        self.max_codes = max_codes

    def __getitem__(self, index):
        # Get a single item as an index from the dataset.
        return {self.feature_name: self.dataset[index, :]}

    def __len__(self):
        # Return the length of the dataset.
        if self.max_codes is None:
            return self.dataset.shape[0]
        else:
            return min(self.dataset.shape[0], self.max_codes)

